---
title: "ProjectPrakDS"
author: "niken"
date: "2022-11-25"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

## Including Plots

You can also embed plots, for example:

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.

1. Library yg digunakan
```{r}
library(twitteR) # scrapping data
library(tm) # corpus
library(syuzhet) # labeling
library(caTools) # split data
library(plyr)
library(RTextTools) # create matrix
library(wordcloud) # wordcloud
library(e1071) # naive bayes
library(caret) # confusion matrix
library(stringr) # string split
library(shiny) # menampilkan GUI 
library(stopwords) #mendeteksi stopwords
library(tokenizers) #mendeteksi stopwords
library(dplyr) #memanipulasi data
library(wordcloud) #membuat wordcloud
library(RColorBrewer) #mendeteksi warna untuk wordcloud

set.seed(100)
```


2. Setup Twitter Auth
```{r}
api_key<- "hRKqGHwsKNXRsaR5lWzLMpACr"
api_secret<- "6eWOmAvYU5eDFKnhgRLfMay6WTRgHRo12SI1u0wdxkATXK4AqK"
access_token<- "1595781028207284224-Bi7Y4TH2Mj7JlGRFQVKlvJnvcGiRGe"
access_token_secret<- "ugXhEySGmltdA5lDRPE332cvrGlyqArj9eURfrJGaKHJU"
setup_twitter_oauth(api_key,api_secret,access_token,access_token_secret)
```
3. Scraping
```{r}
#begin scraping a topic
tweetsList = searchTwitter ('pemerintah -filter:retweets', n = 2000, retryOnRateLimit = 2000, lang = "id")

# convert from twList to data frame
tweets <- twListToDF(twList = tweetsList)

#built dataset
write.csv(tweets, file = 'OriginTweet.csv')
```

4. Preprocessing Data
```{r}
DataOri2 <- read.csv("OriginTweet.csv")
# remove spam tweets
DataMentah <- unique(DataOri2$text)

#hapus URL
removeURL <- function(x) gsub("https[^[:space:]]*", "", x)
tweetClean <- lapply(DataMentah, removeURL)

#hapus new line
removeNL <- function(y) gsub("\n", "", y)
tweetClean <- lapply(tweetClean, removeNL)

#hapus koma
removeKoma <- function(y) gsub(",", "", y)
tweetClean <- lapply(tweetClean, removeKoma)

#hapus retweet
removeRT <- function(y) gsub("RT", "", y)
tweetClean <- lapply(tweetClean, removeRT)

#hapus titik
removeTitik <- function(y) gsub(":", "", y)
tweetClean <- lapply(tweetClean, removeTitik)

#hapus titik koma
removeTitikKoma <- function(y) gsub(";", " ", y)
tweetClean <- lapply(tweetClean, removeTitikKoma)

#hapus titik3
removeTitik3 <- function(y) gsub("p.", "", y)
tweetClean <- lapply(tweetClean, removeTitik3)

#hapus &amp
removeAmp <- function(y) gsub("&amp;", "", y)
tweetClean <- lapply(tweetClean, removeAmp)

#hapus mention
removeMention <- function(z) gsub("@\\w+", "", z)
tweetClean <- lapply(tweetClean, removeMention)

# remove nonalphabetical character
removeNonAlpha <- function(x) gsub("[^A-Za-z ]", "", x)
tweetClean <- lapply(tweetClean, removeNonAlpha)

# trim space into one space
tweetClean <- lapply(tweetClean, stripWhitespace)

# text to lowecase
tweetClean <- lapply(tweetClean, tolower)

# remove stop words
myStopwords <- readLines("stopwords_list.txt")
tweetClean <- as.character(tweetClean)
tweetClean <- tokenize_words(tweetClean, stopwords = myStopwords)

dataframe<-data.frame(text=unlist(sapply(tweetClean, `[`)), stringsAsFactors=F)
write.csv(dataframe,file = "dataPreprocessing.csv")
```

5. Data Labelling
```{r}
SemuaKomen <- read.csv("dataPreprocessing.csv", header = TRUE)

#skoring
kata.positif <- scan("positive-words.txt",what="character",comment.char=";")
kata.negatif <- scan("negative-words.txt",what="character",comment.char=";")
score.sentiment = function(SemuaKomen, kata.positif, kata.negatif,
                           .progress='none')
{
  require(stringr)
  scores = laply(SemuaKomen, function(kalimat, kata.positif,
                                    kata.negatif) {
    kalimat = gsub('[[:punct:]]', '', kalimat)
    kalimat = gsub('[[:cntrl:]]', '', kalimat)
    kalimat = gsub('\\d+', '', kalimat)
    kalimat = tolower(kalimat)
    list.kata = str_split(kalimat, '\\s+')
    kata2 = unlist(list.kata)
    positif.matches = match(kata2, kata.positif)
    negatif.matches = match(kata2, kata.negatif)
    positif.matches = !is.na(positif.matches)
    negatif.matches = !is.na(negatif.matches)
    score = sum(positif.matches) - (sum(negatif.matches))
    return(score)
  }, kata.positif, kata.negatif, .progress=.progress )
  scores.df = data.frame(score=scores, text=SemuaKomen)
  return(scores.df)}

hasil = score.sentiment(SemuaKomen$text, kata.positif, kata.negatif)
#mengubah nilai score menjadi sentimen
hasil$klasifikasi<- ifelse(hasil$score<0, "Negatif",ifelse(hasil$score==0,"Netral","Positif"))

#menukar urutan kolom
data <- hasil[c(3,1,2)]

#View(data)
write.csv(data, file = "datalabel.csv")
```

```{r}
data=read.csv("datalabel.csv")
#melihat 6 data teratas dari datalabel.csv
head(data)
library(tidyverse) 
datas = data %>% select(text,klasifikasi)

#melihat 6 data teratas dari kolom text dan klasifikasi pada datalabel.csv
head(datas)

#presentase setiap label klasifikasi
round(prop.table(table(datas$klasifikasi)),2)
```

6. Visualisasi Data Setelah Labelling
```{r}
library(wordcloud) 
library(ggplot2) 
#wordcloud 
require (RColorBrewer)

positive <- subset(datas,klasifikasi=="Positif")
wordcloud(positive$text, max.words = 100, colors = "green")

negative <- subset(datas,klasifikasi=="Negatif") 
wordcloud(negative$text, max.words = 100, colors = "red") 

netral <- subset(datas,klasifikasi=="Netral") 
wordcloud(netral$text, max.words = 100, colors = "blue")
```


7. Split data menjadi data training dan data testing dengan presentase 75% dan 25%
```{r}
set.seed(31)
    split = sample(2,nrow(dataset),prob = c(0.75,0.25),replace = TRUE)
    train_set = dataset[split == 1,]
    test_set = dataset[split == 2,] 
    
    prop.table(table(train_set$Class))
    prop.table(table(test_set$Class))
```


8.Melakukan klasifikasi naive bayes menggunakan data training
```{r}
    library(e1071)
    library(caret)
    control= trainControl(method="repeatedcv", number=10, repeats=2)
    system.time( classifier_nb <- naiveBayes(train_set, train_set$Class, laplace = 1,trControl = control,tuneLength = 7) )
```

9. Evaluasi Model klasifikasi menggunakan data testing
```{r}

    nb_pred = predict(classifier_nb, type = 'class', newdata =  test_set)
    confusionMatrix(nb_pred,test_set$Class)
```


10. Menampilkan data klasifikasi, Bar Plot, dan Wordcloud pada GUI shinny
```{r}
ui <- fluidPage(
    titlePanel("Sentiment Analysis Pemerintah Indonesia di Twitter"),
        mainPanel(
            
            tabsetPanel(type = "tabs",
                        #Bar Plot
                        tabPanel("Bar Plot", plotOutput("scatterplot")), 
                        # Plot
                        tabPanel("Data", DT::dataTableOutput('tbl1')),
                        # Output Data Dalam Tabel
                        tabPanel("Wordcloud", plotOutput("Wordcloud"))
                        )
        )
    )

# SERVER
server <- function(input, output) {
    
    # Output Data
  output$tbl1 = DT::renderDataTable({
    datatabel <-read.csv("datalabel.csv",stringsAsFactors = FALSE)
    DT::datatable(datatabel, options= list(lengthChange = FALSE))
    })
    
    #output Bar Plot
  output$scatterplot <- renderPlot({
    d<-read.csv("datalabel.csv",stringsAsFactors = FALSE) 
    barchart(d$klasifikasi, 
         horizontal = FALSE, 
         main = "Sentiment Analysis", 
         xlab = "Klasifikasi",
         ylab = "Frequency", 
         col = "darkgreen")
    }, height=400)
    
    #output wordcloud
  output$Wordcloud <- renderPlot({
    require (corpus)
    df<-read.csv("dataTweet.csv",stringsAsFactors = FALSE)
    glimpse(df)
    set.seed(20)
    df<-df[sample(nrow(df)),]
    df<-df[sample(nrow(df)),]
    glimpse(df)
    corpus<-Corpus(VectorSource(df$text))
    corpus

  #fungsinya untuk membersihkan data data yang tidak dibutuhkan 
  corpus.clean<-corpus%>%
    tm_map(content_transformer(tolower))%>%
    tm_map(removePunctuation)%>%
    tm_map(removeNumbers)%>%
    tm_map(removeWords, c("wkwk", "tolol", "oyen"))%>%
    #tm_map(removeWords,stopwords(kind="id"))%>%
    tm_map(stripWhitespace)
  
    wordcloud(corpus.clean,min.freq = 4,max.words=100,random.order=F,colors=brewer.pal(8,"Dark2"))
  })
}

shinyApp(ui = ui, server = server)
```